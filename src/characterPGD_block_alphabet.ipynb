{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoge/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import itertools\n",
    "import copy\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import dataset_64\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"alphabet_model64.pth\" #事前学習済みMNISTモデル(重みパラメータ)\n",
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=57600, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=26, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=57600, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=26, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(57600, 128)\n",
    "        self.fc2 = nn.Linear(128, 26)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# MNISTのTest datasetと dataloaderの定義\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5,), (0.5,))\n",
    "#             ])), \n",
    "#         batch_size=1, shuffle=False)\n",
    "\n",
    "# 使うデバイス（CPUかGPUか）の定義\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# ネットワークの初期化\n",
    "model = Net().to(device)\n",
    "print(model)\n",
    "# 訓練済みモデルのロード\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "\n",
    "\n",
    "# モデルを評価モードに設定。本チュートリアルの例では、これはドロップアウト層等を評価モードにするのに必要\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_is_black(image):\n",
    "    if image < torch.Tensor([0]).to(\"cuda\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_is_white(image):\n",
    "    if image >= torch.Tensor([0]).to(\"cuda\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_search(image):\n",
    "    image_copy = copy.deepcopy(image)\n",
    "    for x in range(64):\n",
    "        for y in range(64):\n",
    "            if im_is_black(image[x][y]):\n",
    "                for i in range(-1, 2):\n",
    "                    for j in range(-1, 2):\n",
    "                        if i != 0 and j != 0:\n",
    "                            continue\n",
    "                        elif (x + i != -1) and (x + i != 64) and (y + j != -1) and (y + j != 64):\n",
    "                            if im_is_white(image[x + i][y + j]):\n",
    "                                image_copy[x][y] = torch.Tensor([1]).to(\"cuda\")\n",
    "                                break\n",
    "                    else:\n",
    "                        continue\n",
    "                    break\n",
    "                else:\n",
    "                    image_copy[x][y] = torch.Tensor([0]).to(\"cuda\")\n",
    "            elif im_is_white(image[x][y]):\n",
    "                for i in range(-1, 2):\n",
    "                    for j in range(-1, 2):\n",
    "                        if i != 0 and j != 0:\n",
    "                            continue\n",
    "                        elif (x + i != -1) and (x + i != 64) and (y + j != -1) and (y + j != 64):\n",
    "                            if im_is_black(image[x + i][y + j]):\n",
    "                                image_copy[x][y] = torch.Tensor([1]).to(\"cuda\")\n",
    "                                break\n",
    "                    else:\n",
    "                        continue\n",
    "                    break\n",
    "                else:\n",
    "                    image_copy[x][y] = torch.Tensor([0]).to(\"cuda\")\n",
    "    image_chain = list(itertools.chain.from_iterable(image_copy))\n",
    "    return image_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def attack(data, data_grad, target, loss_old, dirname_pro, chr, count):\n",
    "#     max_index_list = []\n",
    "#     px = 0\n",
    "    \n",
    "#     data_grad_chain = list(itertools.chain.from_iterable(data_grad[0][0]))\n",
    "#     character_coordinate = character_search(data.data[0][0])\n",
    "#     for i in range(4096):\n",
    "#         if character_coordinate[i] == torch.Tensor([0]).to(\"cuda\"):\n",
    "#             data_grad_chain[i] = torch.Tensor([0]).to(\"cuda\")\n",
    "#     for j in max_index_list:\n",
    "#         data_grad_chain[j] = torch.Tensor([0]).to(\"cuda\")\n",
    "#     data_grad_chain_abs = list(map(abs, data_grad_chain))\n",
    "#     all_blocks = []\n",
    "#     for i in range(4):\n",
    "#         for j in range(4):\n",
    "#             block = []\n",
    "#             for k in range(16):\n",
    "#                 for l in range(16):\n",
    "#                     block.append(data_grad_chain_abs[i * 1024 + j * 16 + k * 64 + l])\n",
    "#             block_sum = sum(block)\n",
    "#             all_blocks.append(block_sum)\n",
    "    \n",
    "#     block_max_id = all_blocks.index(max(all_blocks))\n",
    "#     block_max_index = [block_max_id % 4, block_max_id // 4]\n",
    "\n",
    "#     best_px = 0\n",
    "#     for i in range(16):\n",
    "#         for j in range(16):\n",
    "#             if best_px < data_grad_chain_abs[block_max_index[0] * 16 + block_max_index[1] * 1024 + i * 64 + j]:\n",
    "#                 best_px = data_grad_chain_abs[block_max_index[0] * 16 + block_max_index[1] * 1024 + i * 64 + j]\n",
    "    \n",
    "#     attack_list = [best_px]\n",
    "\n",
    "#     for p in attack_list:\n",
    "        \n",
    "#         if px == 0:\n",
    "#             data_grad_chain_abs_before = copy.deepcopy(data_grad_chain_abs)\n",
    "\n",
    "#         for i in range(8):\n",
    "#             c = max([data_grad_chain_abs[data_grad_chain_abs_before.index(p) - 65], data_grad_chain_abs[data_grad_chain_abs_before.index(p) - 64], data_grad_chain_abs[data_grad_chain_abs_before.index(p) - 63], data_grad_chain_abs[data_grad_chain_abs_before.index(p) - 1], data_grad_chain_abs[data_grad_chain_abs_before.index(p)  + 1], data_grad_chain_abs[data_grad_chain_abs_before.index(p) + 63], data_grad_chain_abs[data_grad_chain_abs_before.index(p) + 64], data_grad_chain_abs[data_grad_chain_abs_before.index(p) + 65]])\n",
    "#             print(c)\n",
    "#             grad_index = data_grad_chain_abs.index(c)\n",
    "#             print(\"abcd\")\n",
    "        \n",
    "            \n",
    "#             if (data_grad_chain[grad_index] > 0 and data[0][0][grad_index // 64][grad_index - 64 * (grad_index // 64)] < 0) or (data_grad_chain[grad_index] < 0 and data[0][0][grad_index // 64][grad_index - 64 * (grad_index // 64)] > 0):\n",
    "#                 data.requires_grad = False\n",
    "#                 data[0][0][grad_index // 64][grad_index - 64 * (grad_index // 64)] *= -1\n",
    "#                 data.requires_grad = True\n",
    "#                 output = model(data)\n",
    "#                 loss = F.nll_loss(output, target)\n",
    "#                 if loss < loss_old:\n",
    "#                     data.requires_grad = False\n",
    "#                     data[0][0][grad_index // 64][grad_index - 64 * (grad_index // 64)] *= -1\n",
    "#                     continue\n",
    "#                 # image[0][0][grad_max_index // 64][grad_max_index - 64 * (grad_max_index // 64)] += ((image[0][0][grad_max_index // 64][grad_max_index - 64 * (grad_max_index // 64)] == torch.Tensor([-1]).to(\"cuda\")) * 0.2 - torch.Tensor([0.1]).to(\"cuda\")).squeeze(0)\n",
    "#                 px += 1\n",
    "#                 plt.xticks([], [])\n",
    "#                 plt.yticks([], [])\n",
    "#                 plt.imsave(dirname_pro + chr + \"/{}\".format(count) + \"/\" + \"{}.png\".format(px), data.squeeze().detach().cpu().numpy(), cmap=\"gray\")\n",
    "#                 print(\"px:{}\".format(px))\n",
    "#                 pred = output.max(1, keepdim=True)[1]\n",
    "#                 if pred.item() != target.item():\n",
    "#                     break\n",
    "#                 loss_old = loss\n",
    "#                 max_index_list.append(grad_index)\n",
    "#                 attack_list.append(c)\n",
    "#                 model.zero_grad()\n",
    "#                 # 逆伝播させてモデルの勾配を計算します\n",
    "#                 loss.backward()\n",
    "#                 # データの勾配を取得します\n",
    "#                 data_grad = data.grad.data\n",
    "#                 data_grad_chain = list(itertools.chain.from_iterable(data_grad[0][0]))\n",
    "#                 character_coordinate = character_search(data.data[0][0])\n",
    "#                 for i in range(4096):\n",
    "#                     if character_coordinate[i] == torch.Tensor([0]).to(\"cuda\"):\n",
    "#                         data_grad_chain[i] = torch.Tensor([0]).to(\"cuda\")\n",
    "#                 for j in max_index_list:\n",
    "#                     data_grad_chain[j] = torch.Tensor([0]).to(\"cuda\")\n",
    "#                 data_grad_chain_abs = list(map(abs, data_grad_chain))\n",
    "                \n",
    "#         else:\n",
    "#             data_grad_chain_abs_before = copy.deepcopy(data_grad_chain_abs)\n",
    "#             # data_grad_chain_abs_now = copy.deepcopy(data_grad_chain_abs)\n",
    "#             continue\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # for px in range(1, 4097):\n",
    "#     #     data_grad_chain = list(itertools.chain.from_iterable(data_grad[0][0]))\n",
    "#     #     character_coordinate = character_search(data.data[0][0])\n",
    "#     #     for i in range(4096):\n",
    "#     #         if character_coordinate[i] == torch.Tensor([0]).to(\"cuda\"):\n",
    "#     #             data_grad_chain[i] = torch.Tensor([0]).to(\"cuda\")\n",
    "#     #     for j in max_index_list:\n",
    "#     #         data_grad_chain[j] = torch.Tensor([0]).to(\"cuda\")\n",
    "#     #     grad_sorted = sorted(list(map(abs, data_grad_chain)), reverse=True)\n",
    "#     #     data_grad_chain_abs = list(map(abs, data_grad_chain))\n",
    "#     #     if px == 1:\n",
    "#     #         all_blocks = []\n",
    "#     #         for i in range(4):\n",
    "#     #             for j in range(4):\n",
    "#     #                 block = []\n",
    "#     #                 for k in range(16):\n",
    "#     #                     for l in range(16):\n",
    "#     #                         block.append(data_grad_chain_abs[i * 1024 + j * 16 + k * 64 + l])\n",
    "#     #                 block_sum = sum(block)\n",
    "#     #                 all_blocks.append(block_sum)\n",
    "            \n",
    "#     #         block_max_id = all_blocks.index(max(all_blocks))\n",
    "#     #         block_max_index = [block_max_id % 4, block_max_id // 4]\n",
    "        \n",
    "#     #         best_px = 0\n",
    "#     #         for i in range(16):\n",
    "#     #             for j in range(16):\n",
    "#     #                 if best_px < data_grad_chain_abs[block_max_index[0] * 16 + block_max_index[1] * 1024 + i * 64 + j]:\n",
    "#     #                     best_px = data_grad_chain_abs[block_max_index[0] * 16 + block_max_index[1] * 1024 + i * 64 + j]\n",
    "\n",
    "\n",
    "        \n",
    "#     #     data.requires_grad = False\n",
    "#     #     cnt = 0\n",
    "#     #     for k in grad_sorted:\n",
    "#     #         cnt += 1\n",
    "#     #         grad_max_index = data_grad_chain_abs.index(k)\n",
    "            \n",
    "#     #         if (data_grad_chain[grad_max_index] > 0 and data[0][0][grad_max_index // 64][grad_max_index - 64 * (grad_max_index // 64)] < 0) or (data_grad_chain[grad_max_index] < 0 and data[0][0][grad_max_index // 64][grad_max_index - 64 * (grad_max_index // 64)] > 0):\n",
    "#     #             data[0][0][grad_max_index // 64][grad_max_index - 64 * (grad_max_index // 64)] *= -1\n",
    "#     #             output = model(data)\n",
    "#     #             if F.nll_loss(output, target) < loss_old:\n",
    "#     #                 data[0][0][grad_max_index // 64][grad_max_index - 64 * (grad_max_index // 64)] *= -1\n",
    "#     #                 continue\n",
    "#     #             # image[0][0][grad_max_index // 64][grad_max_index - 64 * (grad_max_index // 64)] += ((image[0][0][grad_max_index // 64][grad_max_index - 64 * (grad_max_index // 64)] == torch.Tensor([-1]).to(\"cuda\")) * 0.2 - torch.Tensor([0.1]).to(\"cuda\")).squeeze(0)\n",
    "#     #             else:\n",
    "#     #                 max_index_list.append(grad_max_index)\n",
    "#     #                 break\n",
    "#     #     else:\n",
    "#     #         stop = 1\n",
    "#     #     perturbed_data, max_index, stop, output = attack(data, data_copy, data_grad, max_index, target, loss)\n",
    "#     #     plt.xticks([], [])\n",
    "#     #     plt.yticks([], [])\n",
    "#     #     plt.imsave(dirname_pro + chr_lambda(init_pred.item()) + \"/{}\".format(count_list[init_pred.item()]) + \"/\" + \"{}.png\".format(px), perturbed_data.squeeze().detach().cpu().numpy(), cmap=\"gray\")\n",
    "#     #     print(\"px:{}\".format(px))\n",
    "\n",
    "#     #     pred = output.max(1, keepdim=True)[1]\n",
    "#     #     data = perturbed_data\n",
    "#     #     if pred.item() != target.item():\n",
    "#     #         break\n",
    "#     #     elif stop == 1:\n",
    "#     #         break\n",
    "\n",
    "#     #     data.requires_grad = True\n",
    "#     #     # 損失を計算します\n",
    "#     #     loss = F.nll_loss(output, target)\n",
    "#     #     print(\"loss:{}\".format(loss))\n",
    "#     #     # 既存の勾配を全てゼロにします\n",
    "#     #     model.zero_grad()\n",
    "#     #     # 逆伝播させてモデルの勾配を計算します\n",
    "#     #     loss.backward()\n",
    "#     #     # データの勾配を取得します\n",
    "#     #     data_grad = data.grad.data\n",
    "#     return data, pred\n",
    "        \n",
    "#     # return image, max_index_list, stop, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(data, data_grad, target, loss_old, dirname_pro, chr, count):\n",
    "    max_index_list = []\n",
    "    px = 0\n",
    "    \n",
    "    data_grad_chain = list(itertools.chain.from_iterable(data_grad[0][0]))\n",
    "    character_coordinate = character_search(data.data[0][0])\n",
    "    for i in range(4096):\n",
    "        if character_coordinate[i] == torch.Tensor([0]).to(\"cuda\"):\n",
    "            data_grad_chain[i] = torch.Tensor([0]).to(\"cuda\")\n",
    "    for j in max_index_list:\n",
    "        data_grad_chain[j] = torch.Tensor([0]).to(\"cuda\")\n",
    "    data_grad_chain_abs = list(map(abs, data_grad_chain))\n",
    "    all_blocks = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            block = []\n",
    "            for k in range(16):\n",
    "                for l in range(16):\n",
    "                    block.append(data_grad_chain_abs[i * 1024 + j * 16 + k * 64 + l])\n",
    "            block_sum = sum(block)\n",
    "            all_blocks.append(block_sum)\n",
    "    \n",
    "    # block_max1_id = all_blocks.index(max(all_blocks))\n",
    "    # block_max1_index = [block_max1_id % 4, block_max1_id // 4]\n",
    "    # all_blocks.remove(max(all_blocks))\n",
    "    # block_max2_id = all_blocks.index(max(all_blocks))\n",
    "    # block_max2_index = [block_max2_id % 4, block_max2_id // 4]\n",
    "\n",
    "    block_max_index_list = []\n",
    "    for i in sorted(all_blocks, reverse=True):\n",
    "        if i== torch.Tensor([0]).to(\"cuda\"):\n",
    "            break\n",
    "        block_max_id = all_blocks.index(i)\n",
    "        block_max_index_list.append([block_max_id % 4, block_max_id // 4])\n",
    "\n",
    "    for ind, block_max_index in enumerate(block_max_index_list):\n",
    "        print(\"block:{}\".format(ind))\n",
    "        best_px = 0\n",
    "        for i in range(16):\n",
    "            for j in range(16):\n",
    "                if best_px < data_grad_chain_abs[block_max_index[0] * 16 + block_max_index[1] * 1024 + i * 64 + j]:\n",
    "                    best_id = block_max_index[0] * 16 + block_max_index[1] * 1024 + i * 64 + j\n",
    "        \n",
    "        if (data_grad_chain[best_id] > 0 and data[0][0][best_id // 64][best_id - 64 * (best_id // 64)] < 0) or (data_grad_chain[best_id] < 0 and data[0][0][best_id // 64][best_id - 64 * (best_id // 64)] > 0):\n",
    "            data.requires_grad = False\n",
    "            data[0][0][best_id // 64][best_id - 64 * (best_id // 64)] *= -1\n",
    "            data.requires_grad = True\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            if loss < loss_old:\n",
    "                data.requires_grad = False\n",
    "                data[0][0][best_id // 64][best_id - 64 * (best_id // 64)] *= -1\n",
    "            # image[0][0][grad_max_index // 64][grad_max_index - 64 * (grad_max_index // 64)] += ((image[0][0][grad_max_index // 64][grad_max_index - 64 * (grad_max_index // 64)] == torch.Tensor([-1]).to(\"cuda\")) * 0.2 - torch.Tensor([0.1]).to(\"cuda\")).squeeze(0)\n",
    "            else:\n",
    "                px += 1\n",
    "                plt.xticks([], [])\n",
    "                plt.yticks([], [])\n",
    "                plt.imsave(dirname_pro + chr + \"/{}\".format(count) + \"/\" + \"{}.png\".format(px), data.squeeze().detach().cpu().numpy(), cmap=\"gray\")\n",
    "                print(\"px:{}\".format(px))\n",
    "                print(\"loss:{}\".format(loss))\n",
    "                pred = output.max(1, keepdim=True)[1]\n",
    "                if pred.item() != target.item():\n",
    "                    return data, pred\n",
    "                loss_old = loss\n",
    "                max_index_list.append(best_id)\n",
    "                model.zero_grad()\n",
    "                # 逆伝播させてモデルの勾配を計算します\n",
    "                loss.backward()\n",
    "                # データの勾配を取得します\n",
    "                data_grad = data.grad.data\n",
    "                data_grad_chain = list(itertools.chain.from_iterable(data_grad[0][0]))\n",
    "                character_coordinate = character_search(data.data[0][0])\n",
    "                for i in range(4096):\n",
    "                    if character_coordinate[i] == torch.Tensor([0]).to(\"cuda\"):\n",
    "                        data_grad_chain[i] = torch.Tensor([0]).to(\"cuda\")\n",
    "                for j in max_index_list:\n",
    "                    data_grad_chain[j] = torch.Tensor([0]).to(\"cuda\")\n",
    "                data_grad_chain_abs = list(map(abs, data_grad_chain))\n",
    "        \n",
    "        attack_list = [best_id]\n",
    "\n",
    "        for p in attack_list:\n",
    "            p_list = [p - 65, p - 64, p - 63, p - 1, p + 1, p + 63, p + 64, p + 65] \n",
    "            for i in range(8):\n",
    "                p_max = -1\n",
    "                for j in p_list:\n",
    "                    if p_max < data_grad_chain_abs[j]:\n",
    "                        p_max = data_grad_chain_abs[j]\n",
    "                        p_max_id = j\n",
    "                p_list.remove(p_max_id)\n",
    "                \n",
    "                if (data_grad_chain[p_max_id] > 0 and data[0][0][p_max_id // 64][p_max_id - 64 * (p_max_id // 64)] < 0) or (data_grad_chain[p_max_id] < 0 and data[0][0][p_max_id // 64][p_max_id - 64 * (p_max_id // 64)] > 0):\n",
    "                    data.requires_grad = False\n",
    "                    data[0][0][p_max_id // 64][p_max_id - 64 * (p_max_id // 64)] *= -1\n",
    "                    data.requires_grad = True\n",
    "                    output = model(data)\n",
    "                    loss = F.nll_loss(output, target)\n",
    "                    if loss < loss_old:\n",
    "                        data.requires_grad = False\n",
    "                        data[0][0][p_max_id // 64][p_max_id - 64 * (p_max_id // 64)] *= -1\n",
    "                        continue\n",
    "                    # image[0][0][grad_max_index // 64][grad_max_index - 64 * (grad_max_index // 64)] += ((image[0][0][grad_max_index // 64][grad_max_index - 64 * (grad_max_index // 64)] == torch.Tensor([-1]).to(\"cuda\")) * 0.2 - torch.Tensor([0.1]).to(\"cuda\")).squeeze(0)\n",
    "                    px += 1\n",
    "                    plt.xticks([], [])\n",
    "                    plt.yticks([], [])\n",
    "                    plt.imsave(dirname_pro + chr + \"/{}\".format(count) + \"/\" + \"{}.png\".format(px), data.squeeze().detach().cpu().numpy(), cmap=\"gray\")\n",
    "                    print(\"px:{}\".format(px))\n",
    "                    print(\"loss:{}\".format(loss))\n",
    "                    pred = output.max(1, keepdim=True)[1]\n",
    "                    if pred.item() != target.item():\n",
    "                        return data, pred\n",
    "                    loss_old = loss\n",
    "                    max_index_list.append(p_max_id)\n",
    "                    attack_list.append(p_max_id)\n",
    "                    model.zero_grad()\n",
    "                    # 逆伝播させてモデルの勾配を計算します\n",
    "                    loss.backward()\n",
    "                    # データの勾配を取得します\n",
    "                    data_grad = data.grad.data\n",
    "                    data_grad_chain = list(itertools.chain.from_iterable(data_grad[0][0]))\n",
    "                    character_coordinate = character_search(data.data[0][0])\n",
    "                    for i in range(4096):\n",
    "                        if character_coordinate[i] == torch.Tensor([0]).to(\"cuda\"):\n",
    "                            data_grad_chain[i] = torch.Tensor([0]).to(\"cuda\")\n",
    "                    for j in max_index_list:\n",
    "                        data_grad_chain[j] = torch.Tensor([0]).to(\"cuda\")\n",
    "                    data_grad_chain_abs = list(map(abs, data_grad_chain))\n",
    "    return data, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device):\n",
    "    dir = \"test\"\n",
    "    chr_lambda = lambda a: chr(a + 65)\n",
    "    # dirname_grad = \"../PGD_alphabet_\" + dir + \"/grad/\"\n",
    "    # dirname_org = \"../PGD_alphabet_\" + dir + \"/org/\"\n",
    "    # dirname_adv = \"../PGD_alphabet_\" + dir + \"/adv/\"\n",
    "    # dirname_pro = \"../PGD_alphabet_\" + dir + \"/progress/\"\n",
    "    dirname_grad = \"../PGD_alphabet_\" + dir + \"_upd2\" + \"/grad/\"\n",
    "    dirname_org = \"../PGD_alphabet_\" + dir + \"_upd2\" + \"/org/\"\n",
    "    dirname_adv = \"../PGD_alphabet_\" + dir + \"_upd2\" + \"/adv/\"\n",
    "    dirname_pro = \"../PGD_alphabet_\" + dir + \"_upd2\" + \"/progress/\"\n",
    "    for c in [chr(i) for i in range(65, 65+26)]:\n",
    "        os.makedirs(dirname_grad + c, exist_ok=True)\n",
    "        os.makedirs(dirname_org + c, exist_ok=True)\n",
    "        os.makedirs(dirname_adv + c, exist_ok=True)\n",
    "        os.makedirs(dirname_pro + c, exist_ok=True)\n",
    "        for d in [chr(i) for i in range(65, 65+26)]:\n",
    "            os.makedirs(dirname_adv + c + \"/\" + c + \"→\" + d, exist_ok=True)\n",
    "        for e in range(1, 401):\n",
    "            os.makedirs(dirname_pro + c + \"/{}\".format(e), exist_ok=True)\n",
    "    \n",
    "    datas = torch.from_numpy(dataset_64.X_test.astype(np.float32)).clone()\n",
    "    labels = torch.from_numpy(dataset_64.y_test.astype(np.float32)).clone()\n",
    "\n",
    "    # 精度カウンター\n",
    "    correct = 0\n",
    "    # count = 0\n",
    "    count_list = [0] * 26\n",
    "\n",
    "    adv_examples = []\n",
    "    # i=0\n",
    "    \n",
    "\n",
    "    # テスト用データセット内の全てのサンプルをループします\n",
    "    for data, target in zip(datas, labels):\n",
    "        \n",
    "        # i+=1\n",
    "        # if i==150:\n",
    "        #     break\n",
    "        # データとラベルをデバイス（CPUもしくはGPU）に送信します\n",
    "        data, target = data.to(device), target.type(torch.LongTensor).to(device)\n",
    "        # テンソルの requires_grad 属性を設定します。攻撃者にとっては重要な設定です。\n",
    "        data.requires_grad = True\n",
    "\n",
    "        # データをモデルに順伝播させます\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # 最大の確率のインデックスを取得します。\n",
    "\n",
    "        # 最初から予測が間違っている場合、攻撃する必要がないため次のイテレーションに進みます。\n",
    "        \n",
    "        if init_pred.item() != target.item() or init_pred.item() != 8:\n",
    "            continue\n",
    "\n",
    "        data_copy = data.detach().clone()\n",
    "        # character_coordinate = character_search(data_copy.data[0][0])\n",
    "        \n",
    "        count_list[init_pred.item()] += 1\n",
    "        \n",
    "    \n",
    "        # 損失を計算します\n",
    "        loss = F.nll_loss(output, target)\n",
    "        print(\"loss:{}\".format(loss))\n",
    "        print(type(target))\n",
    "        break\n",
    "        # 既存の勾配を全てゼロにします\n",
    "        model.zero_grad()\n",
    "        # 逆伝播させてモデルの勾配を計算します\n",
    "        loss.backward()\n",
    "        # データの勾配を取得します\n",
    "        data_grad = data.grad.data\n",
    "        \n",
    "        max_index = []\n",
    "        #勾配のヒートマップ\n",
    "        grad_map = data_grad.squeeze().detach().cpu().numpy()\n",
    "        grad_map_abs = np.abs(grad_map)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        plt.imsave(dirname_grad + chr_lambda(init_pred.item()) + \"/{}.png\".format(count_list[init_pred.item()]), grad_map_abs, cmap=\"Reds\")\n",
    "\n",
    "        perturbed_data, pred = attack(data, data_grad, target, loss, dirname_pro, chr_lambda(init_pred.item()), count_list[init_pred.item()])\n",
    "        \n",
    "        final_pred = pred\n",
    "\n",
    "        org = data_copy.squeeze().detach().cpu().numpy()\n",
    "        adv = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "\n",
    "        #各条件を満たす画像の保存\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        print(dirname_org + chr_lambda(init_pred.item()) + \"/{}.png\".format(count_list[init_pred.item()]))\n",
    "        plt.imsave(dirname_org + chr_lambda(init_pred.item()) + \"/{}.png\".format(count_list[init_pred.item()]), org, cmap=\"gray\")\n",
    "        \n",
    "        os.makedirs(dirname_adv + chr_lambda(init_pred.item()) + \"/\" + chr_lambda(init_pred.item()) + \"→\" + chr_lambda(final_pred.item()) + \"/\", exist_ok=True)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        plt.imsave(dirname_adv + chr_lambda(init_pred.item()) + \"/\" + chr_lambda(init_pred.item()) + \"→\" + chr_lambda(final_pred.item()) + \"/{}.png\".format(count_list[init_pred.item()]), adv, cmap=\"gray\")\n",
    "\n",
    "        if count_list[init_pred.item()] == 50:\n",
    "            break\n",
    "\n",
    "        if (final_pred.item() != target.item()):\n",
    "            # あとで可視化するために敵対的サンプルのうちいくつかを保存\n",
    "            if len(adv_examples) < 25:\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), org, adv))\n",
    "                # if len(adv_examples) == 25:\n",
    "                #     break\n",
    "\n",
    "\n",
    "        # # epsilonごとの最終的な精度を算出\n",
    "        # final_acc = correct/float(len(test_loader))\n",
    "        # print(\"Test Accuracy = {} / {} = {}\".format(correct, len(test_loader), final_acc))\n",
    "\n",
    "        # # 精度と敵対的サンプルを返却\n",
    "        # return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.00016175392374861985\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mセル10 を /workspace/src/PGD_alphabet2.ipynb\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/PGD_alphabet2.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m examples \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/PGD_alphabet2.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 各epsilonごとにテストを実行\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/PGD_alphabet2.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m acc, ex \u001b[39m=\u001b[39m test(model, device)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/PGD_alphabet2.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m accuracies\u001b[39m.\u001b[39mappend(acc)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/PGD_alphabet2.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m examples\u001b[39m.\u001b[39mappend(ex)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# 各epsilonごとにテストを実行\n",
    "acc, ex = test(model, device)\n",
    "accuracies.append(acc)\n",
    "examples.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mセル11 を /workspace/src/PGD_alphabet2.ipynb\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/PGD_alphabet2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m cnt \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/PGD_alphabet2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/PGD_alphabet2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(examples[\u001b[39m0\u001b[39;49m])):\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/PGD_alphabet2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     cnt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/PGD_alphabet2.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     plt\u001b[39m.\u001b[39msubplot(\u001b[39m5\u001b[39m,\u001b[39m5\u001b[39m,cnt)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt = 0\n",
    "plt.figure(figsize=(8,10))\n",
    "\n",
    "for j in range(len(examples[0])):\n",
    "    cnt += 1\n",
    "    plt.subplot(5,5,cnt)\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    orig,adv, org_ex, adv_ex = examples[0][j]\n",
    "    plt.title(\"{}\".format(orig), color=\"white\")\n",
    "    plt.imshow(org_ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "plt.figure(figsize=(8,10))\n",
    "\n",
    "for j in range(len(examples[0])):\n",
    "    cnt += 1\n",
    "    plt.subplot(5,5,cnt)\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    orig, adv, org_ex, adv_ex = examples[0][j]\n",
    "    plt.title(\"{} -> {}\".format(orig, adv), color=\"white\")\n",
    "    plt.imshow(adv_ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
