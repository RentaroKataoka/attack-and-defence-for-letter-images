{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoge/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import csv\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabet_list = [[30, 600, 1568, 709, 802, 816, 1258, 426], [579, 867, 1385, 53, 356, 431, 669, 763], [166, 245, 844, 1065, 1255, 1286, 137], [288, 1056, 1123, 1160, 1268, 920, 1039, 1508], [1201, 1225, 542, 1016, 150, 223, 254, 431], [1129, 1879, 66, 271, 262, 732, 807], [466, 105, 1084, 91, 251, 789, 1542, 1935], [1690, 1902, 311, 371, 970, 1514, 1538, 1924], [7, 255, 400, 751, 794, 876, 22, 92], [124, 187, 896, 1180, 1250, 1587, 1604, 1635], [1502, 100, 181, 1231, 1234, 80, 245, 409], [1569, 363, 1101, 1316, 1439, 1863, 730, 832], [291, 133, 173, 274, 904, 1023, 1085, 1449], [151, 149, 587, 820, 1099, 1459, 1483, 1698], [105, 1270, 1576, 32, 405, 831, 968, 1527], [310, 934, 1976, 288, 400, 689, 807, 1536], [396, 862, 1105, 45, 170, 495, 791, 1064], [1182, 784, 1096, 18, 867, 918, 968, 971], [1487, 1569, 772, 182, 281, 414, 447, 1775], [142, 623, 145, 182, 1831, 148, 214, 1466], [1174, 13, 274, 533, 745, 1277, 1325, 1875], [1009, 51, 113, 245, 482, 730, 1344, 1633], [1351, 340, 978, 22, 1698, 1943, 322, 777], [282, 714, 1384, 1516, 1557, 1610, 90, 778], [687, 668, 930, 1673, 1956, 32, 53, 110], [1992, 103, 188, 501, 892, 1558, 1660, 1931]]\n",
    "# for i, alpha in enumerate(alphabet_list):\n",
    "#     os.makedirs(\"../GAN+classifier_c1e-6/strong/\" + chr(i + 65), exist_ok=True)\n",
    "#     images = []\n",
    "#     for j in alpha:\n",
    "#         img_array = cv2.imread(\"../GAN+classifier_c1e-6/org/\" + chr(i + 65) + \"/{}.png\".format(j), cv2.IMREAD_GRAYSCALE)  # 画像読み込み\n",
    "#         plt.xticks([], [])\n",
    "#         plt.yticks([], [])\n",
    "#         plt.imsave(\"../GAN+classifier_c1e-6/strong/\" + chr(i + 65) + \"/{}.png\".format(j), img_array, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def torch_fix_seed(seed=42):\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms = True\n",
    "\n",
    "\n",
    "torch_fix_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.11.0+cu113\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('torch version:',torch.__version__)\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64 # Image size\n",
    "batch_size = 16  # Batch size\n",
    "\n",
    "# Model\n",
    "z_size = 100\n",
    "ngf = 32\n",
    "ndf = 64\n",
    "generator_layer_size = [256, 512, 1024, 2048]\n",
    "discriminator_layer_size = [2048, 1024, 512, 256]\n",
    "\n",
    "# Training\n",
    "epochs = 100 # Train epochs\n",
    "learning_rate_G = 2e-5\n",
    "learning_rate_D = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = [chr(i + 65) for i in range(26)]\n",
    "class_num = len(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(label, n_class=26):  \n",
    "    # ラベルをOne-Hoe形式に変換\n",
    "    eye = torch.eye(n_class).to(device)\n",
    "    # ランダムベクトルあるいは画像と連結するために(B, c_class, 1, 1)のTensorにして戻す\n",
    "    return eye[label].view(-1, n_class, 1, 1)\n",
    "    \n",
    "def concat_image_label(image, label, n_class=26):\n",
    "    # 画像とラベルを連結する\n",
    "    oh_label = onehot_encode(label, n_class).to(device)       # ラベルをOne-Hot形式に変換\n",
    "    oh_label = oh_label.expand(16, n_class, 64, 64)  # ラベルを画像サイズに拡大\n",
    "    return torch.cat((image, oh_label), dim=1)    # 画像とラベルをチャネル方向（dim=1）で連結\n",
    " \n",
    "def concat_noise_label(noise, label, n_class=26):\n",
    "    # ランダムベクトルとラベルを連結する\n",
    "    oh_label = onehot_encode(label, n_class).to(device)     # ラベルをOne-Hot形式に変換\n",
    "    return torch.cat((noise, oh_label), dim=1)  # ランダムベクトルとラベルを連結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, generator_layer_size, z_size, img_size, class_num):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.z_size = z_size\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # self.label_emb = nn.Embedding(class_num, class_num)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.z_size + 26, ngf * 32, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 32, ngf * 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, 1, 3, 1, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        \n",
    "        # Reshape z\n",
    "        # z = z.view(-1, self.z_size)\n",
    "        \n",
    "        # One-hot vector to embedding vector\n",
    "        # c = self.label_emb(labels)\n",
    "        \n",
    "        # Concat image & label\n",
    "        # x = torch.cat([z, c], 1)\n",
    "\n",
    "        x = concat_noise_label(z, labels)\n",
    "\n",
    "\n",
    "        # rlabels_list = []\n",
    "        # for label in rlabels:\n",
    "        #     l = torch.Tensor([[[label.item()]]])\n",
    "        #     rlabels_list.append(l)\n",
    "        # rlabels_list = torch.stack(rlabels_list).to(device)\n",
    "\n",
    "        # x = torch.cat((x, rlabels_list), dim=1)\n",
    "        \n",
    "        # Generator out\n",
    "        out = self.model(x)\n",
    "        \n",
    "        return out\n",
    "        # return out.view(-1, self.img_size, self.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = Generator(generator_layer_size, z_size, img_size, class_num).to(device)\n",
    "generator2 = Generator(generator_layer_size, z_size, img_size, class_num).to(device)\n",
    "# generator.load_state_dict(torch.load(\"generator+reg_hinge20.pth\", map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=57600, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=26, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=57600, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=26, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model = \"googlefonts.pth\"\n",
    "use_cuda = True\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(57600, 128)\n",
    "        self.fc2 = nn.Linear(128, 26)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "# MNISTのTest datasetと dataloaderの定義\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5,), (0.5,))\n",
    "#             ])), \n",
    "#         batch_size=1, shuffle=False)\n",
    "\n",
    "# 使うデバイス（CPUかGPUか）の定義\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# ネットワークの初期化\n",
    "model = Net().to(device)\n",
    "print(model)\n",
    "# 訓練済みモデルのロード\n",
    "model.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(57600, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# 使うデバイス（CPUかGPUか）の定義\n",
    "rmodel_list = []\n",
    "for i in range(26):\n",
    "    pretrained_model = \"../GoogleFonts_reg/model/reg_\" + chr(i + 65) + \".pth\"\n",
    "    # ネットワークの初期化\n",
    "    rmodel = rNet().to(device)\n",
    "    # 訓練済みモデルのロード\n",
    "    rmodel.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n",
    "    # モデルを評価モードに設定。本チュートリアルの例では、これはドロップアウト層等を評価モードにするのに必要\n",
    "    rmodel.eval()\n",
    "    rmodel_list.append(rmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform():\n",
    "  def __init__(self, mean, std):\n",
    "    self.data_transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Normalize(mean, std)\n",
    "    ])\n",
    "\n",
    "  def __call__(self, img):\n",
    "    return self.data_transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.5,)\n",
    "std = (0.5,)\n",
    "images = ImageFolder( \"../GoogleFonts/all\", transform = ImageTransform(mean, std))\n",
    "data_loader = DataLoader(images, batch_size=500, shuffle = True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(data, data_grad, target, epsilon, lim):\n",
    "    for i in range(1, 1001):\n",
    "        data.requires_grad = False\n",
    "        sign_data_grad = data_grad.sign()\n",
    "        perturbed_data = data + epsilon * sign_data_grad\n",
    "        perturbed_data = torch.clamp(perturbed_data, -1, 1)\n",
    "        data = perturbed_data\n",
    "        data.requires_grad = True\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        if pred.item() != target.item():\n",
    "            break\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = data.grad.data\n",
    "    return data, pred, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "24.748\n",
      "B\n",
      "19.069\n",
      "C\n",
      "22.068\n",
      "D\n",
      "21.261\n",
      "E\n",
      "18.566\n",
      "F\n",
      "23.937\n",
      "G\n",
      "22.776\n",
      "H\n",
      "23.873\n",
      "I\n",
      "22.569\n",
      "J\n",
      "23.195\n",
      "K\n",
      "21.318\n",
      "L\n",
      "25.114\n",
      "M\n",
      "19.475\n",
      "N\n",
      "22.372\n",
      "O\n",
      "16.564\n",
      "P\n",
      "22.221\n",
      "Q\n",
      "22.988\n",
      "R\n",
      "20.798\n",
      "S\n",
      "21.148\n",
      "T\n",
      "22.827\n",
      "U\n",
      "19.02\n",
      "V\n",
      "20.303\n",
      "W\n",
      "23.418\n",
      "X\n",
      "16.652\n",
      "Y\n",
      "23.852\n",
      "Z\n",
      "25.04\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1, 101):\n",
    "#     os.makedirs(\"../GAN+classifier_c1e-6/hist/{}\".format(i), exist_ok=True)\n",
    "#     os.makedirs(\"../GAN+classifier_c1e-7/hist/{}\".format(i), exist_ok=True)\n",
    "\n",
    "\n",
    "# all_list1 = []\n",
    "# all_list2 = []\n",
    "# for para in range(1, 101):\n",
    "#     generator1.load_state_dict(torch.load(\"../GAN+classifier_c1e-6/model/{}.pth\".format(para), map_location='cpu'))\n",
    "#     generator2.load_state_dict(torch.load(\"../GAN+classifier_c1e-7/model/{}.pth\".format(para), map_location='cpu'))\n",
    "#     if para == 1:\n",
    "#         all_real = []\n",
    "#         all_real_mean = []\n",
    "plt.rcParams[\"font.size\"] = 25\n",
    "real_hist_mean = []\n",
    "real_hist_var = []\n",
    "hist_mean = []\n",
    "hist_var = []\n",
    "all_real_list = []\n",
    "all_list1 = []\n",
    "for alphabet in range(26):\n",
    "\n",
    "    print(chr(alphabet + 65))\n",
    "    # generator1.load_state_dict(torch.load(\"../GAN+classifier_c1e-6/model/18.pth\", map_location='cpu'))\n",
    "    generator1.load_state_dict(torch.load(\"../GAN+classifier_c1e-6/model/18.pth\", map_location='cpu'))\n",
    "\n",
    "    l1 = os.listdir(\"../normalPGD_googlefonts/progress/\" + chr(alphabet + 65))\n",
    "    real_list = []\n",
    "    for j in l1:\n",
    "        all_real_list.append(sum(os.path.isfile(os.path.join(\"../normalPGD_googlefonts/progress/\" + chr(alphabet + 65) + \"/\" + j, name)) for name in os.listdir(\"../normalPGD_googlefonts/progress/\" + chr(alphabet + 65) + \"/\" + j)))\n",
    "        real_list.append(sum(os.path.isfile(os.path.join(\"../normalPGD_googlefonts/progress/\" + chr(alphabet + 65) + \"/\" + j, name)) for name in os.listdir(\"../normalPGD_googlefonts/progress/\" + chr(alphabet + 65) + \"/\" + j)))\n",
    "        if len(real_list) == 1000:\n",
    "            # all_real_mean.append(np.mean(real_list))\n",
    "            # all_real.append(real_list)\n",
    "            break\n",
    "\n",
    "    real_hist_mean.append(np.mean(real_list))\n",
    "    real_hist_var.append(np.var(real_list))\n",
    "    # print(np.mean(real_list))\n",
    "    # plt.ylim(0, 200)\n",
    "    # plt.hist(real_list, range=(0, 40), bins = 40, alpha = 0.5, label=\"real images\")\n",
    "    # plt.xlabel(\"estimated resistance\")\n",
    "    # plt.ylabel(\"samples\")\n",
    "    # plt.legend(loc='upper left')\n",
    "    # plt.savefig(\"../normalPGD_googlefonts/hist/hist_\" + chr(alphabet + 65) + \".png\", facecolor=\"white\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    z = Variable(torch.randn(1200, z_size, 1, 1)).to(device)\n",
    "    # Labels 0 ~ 9\n",
    "    labels = Variable(torch.LongTensor(np.zeros(1200) + alphabet)).to(device)\n",
    "    # Generating images\n",
    "    sample_images1 = generator1(z, labels).data.to(\"cpu\")\n",
    "    # sample_images2 = generator2(z, labels).data.to(\"cpu\")\n",
    "\n",
    "    list1 = []\n",
    "    false = 0\n",
    "    for image, label in zip(sample_images1, labels):\n",
    "        image, label = image.unsqueeze(0).to(device), label.unsqueeze(0).type(torch.LongTensor).to(device)\n",
    "        image.requires_grad = True\n",
    "        # データをモデルに順伝播させます\n",
    "        output = model(image)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # 最大の確率のインデックスを取得します。\n",
    "        # 最初から予測が間違っている場合、攻撃する必要がないため次のイテレーションに進みます。\n",
    "        if init_pred.item() != label.item():\n",
    "            continue\n",
    "        # 損失を計算します\n",
    "        loss = F.nll_loss(output, label)\n",
    "        # 既存の勾配を全てゼロにします\n",
    "        model.zero_grad()\n",
    "        # 逆伝播させてモデルの勾配を計算します\n",
    "        loss.backward()\n",
    "        # データの勾配を取得します\n",
    "        data_grad = image.grad.data\n",
    "        perturbed_data, pred, resistance = attack(image, data_grad, label, 0.02, 0)\n",
    "        list1.append(resistance)\n",
    "        all_list1.append(resistance)\n",
    "        if len(list1) == 1000:\n",
    "            # all_list1.append(np.mean(list1))\n",
    "            break\n",
    "\n",
    "    print(np.mean(list1))\n",
    "    hist_mean.append(np.mean(list1))\n",
    "    hist_var.append(np.var(list1))\n",
    "    # plt.figure(figsize=(12, 8))\n",
    "    # plt.xlim(0, 40)\n",
    "    # plt.ylim(0, 400)\n",
    "    # plt.hist(real_list, range=(0, 40), bins = 40, alpha = 0.5, label=\"Real images\")\n",
    "    # plt.hist(list1, range=(0, 40), bins = 40, alpha = 0.5, label=\"Reconstructed images\")\n",
    "    # plt.tick_params(labelbottom=False, labelleft=False, labelright=False, labeltop=False)\n",
    "    # plt.tick_params(bottom=False, left=False, right=False, top=False)\n",
    "    # plt.xticks([k * 10 for k in range(5)])\n",
    "    # plt.yticks([i * 100 for i in range(5)])\n",
    "    # plt.grid()\n",
    "    # # plt.xlabel(\"Defensibility\")\n",
    "    # # plt.ylabel(\"Samples\")\n",
    "    # # plt.legend(loc='upper left')\n",
    "    # plt.savefig(\"../GAN+classifier_c1e-6/hist_\" + chr(alphabet + 65) + \".png\", facecolor=\"white\", bbox_inches='tight', pad_inches=0.1)\n",
    "    # plt.show()\n",
    "\n",
    "    # list2 = []\n",
    "    # false = 0\n",
    "    # for image, label in zip(sample_images2, labels):\n",
    "    #     image, label = image.unsqueeze(0).to(device), label.unsqueeze(0).type(torch.LongTensor).to(device)\n",
    "    #     image.requires_grad = True\n",
    "    #     output = model(image)\n",
    "    #     init_pred = output.max(1, keepdim=True)[1] # 最大の確率のインデックスを取得します。\n",
    "    #     # 最初から予測が間違っている場合、攻撃する必要がないため次のイテレーションに進みます。\n",
    "    #     if init_pred.item() != label.item():\n",
    "    #         continue\n",
    "    #     # 損失を計算します\n",
    "    #     loss = F.nll_loss(output, label)\n",
    "    #     # 既存の勾配を全てゼロにします\n",
    "    #     model.zero_grad()\n",
    "    #     # 逆伝播させてモデルの勾配を計算します\n",
    "    #     loss.backward()\n",
    "    #     # データの勾配を取得します\n",
    "    #     data_grad = image.grad.data\n",
    "    #     perturbed_data, pred, resistance = attack(image, data_grad, label, 0.02, 0)\n",
    "    #     list2.append(resistance)\n",
    "    #     if len(list2) == 1000:\n",
    "    #         all_list2.append(np.mean(list2))\n",
    "    #         break\n",
    "    # print(np.mean(list2))\n",
    "    # print(len(list2))\n",
    "    # plt.ylim(0, 200)\n",
    "    # plt.hist(list2, range=(0, 50), bins = 50, alpha = 0.5, label=\"1e-7\")\n",
    "    # plt.xlabel(\"estimated resistance\")\n",
    "    # plt.ylabel(\"samples\")\n",
    "    # plt.legend(loc='upper left')\n",
    "    # plt.savefig(\"../GAN+classifier_c1e-7/hist/{}/hist_\".format(para) + chr(alphabet + 65) + \".png\", facecolor=\"white\")\n",
    "    # plt.show()\n",
    "        \n",
    "    # if para == 1:\n",
    "    #     with open(\"../normalPGD_googlefonts/real.csv\", 'w', newline='') as file:\n",
    "    #         writer = csv.writer(file)\n",
    "    #         writer.writerows(all_real)\n",
    "    #         file.close()\n",
    "# with open(\"../GAN+classifier_c1e-6/hist_mean_real.csv\", 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(real_hist_mean)\n",
    "#     file.close()\n",
    "\n",
    "with open(\"../GAN+classifier_c1e-6/original_image_mean_var.csv\", 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([real_hist_mean, real_hist_var])\n",
    "    file.close()\n",
    "\n",
    "with open(\"../GAN+classifier_c1e-6/GAN+classifier_mean_var.csv\", 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows([hist_mean, hist_var])\n",
    "    file.close()\n",
    "\n",
    "\n",
    "    \n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.xlim(0, 40)\n",
    "# plt.ylim(0, 2000)\n",
    "# plt.hist(all_real_list, range=(0, 40), bins = 40, alpha = 0.5, label=\"Real images\")\n",
    "# plt.hist(all_list1, range=(0, 40), bins = 40, alpha = 0.5, label=\"Reconstructed images\")\n",
    "# plt.tick_params(labelbottom=False, labelleft=False, labelright=False, labeltop=False)\n",
    "# plt.tick_params(bottom=False, left=False, right=False, top=False)\n",
    "# plt.xticks([k * 10 for k in range(5)])\n",
    "# plt.yticks([i * 100 for i in range(5)])\n",
    "# plt.savefig(\"../GAN+classifier_c1e-6/hist_all.png\", facecolor=\"white\", bbox_inches='tight', pad_inches=0.1)\n",
    "# plt.show() \n",
    "\n",
    "    # print(chr(alphabet + 65))\n",
    "\n",
    "    # print(np.mean(real_list))\n",
    "    # print(len(real_list))\n",
    "    # plt.ylim(0, 120)\n",
    "    # plt.hist(real_list, range=(0, 40), bins = 30, alpha = 0.5, label=\"real images\")\n",
    "    # plt.xlabel(\"estimated resistance\")\n",
    "    # plt.ylabel(\"samples\")\n",
    "    # plt.legend(loc='upper left')\n",
    "    # plt.savefig(\"hist_{}.png\", facecolor=\"white\")\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # print(np.mean(list1))\n",
    "    # print(len(list1))\n",
    "    # plt.ylim(0, 120)\n",
    "    # plt.hist(list1, range=(0, 40), bins = 30, alpha = 0.5, label=\"1e-6\")\n",
    "    # plt.xlabel(\"estimated resistance\")\n",
    "    # plt.ylabel(\"samples\")\n",
    "    # plt.legend(loc='upper left')\n",
    "    # # plt.savefig(\"GAN_hist_real_gen.png\", facecolor=\"white\")\n",
    "    # plt.show()\n",
    "\n",
    "    # print(np.mean(list2))\n",
    "    # print(len(list2))\n",
    "    # plt.ylim(0, 120)\n",
    "    # plt.hist(list2, range=(0, 40), bins = 30, alpha = 0.5, label=\"1e-7\")\n",
    "    # plt.xlabel(\"estimated resistance\")\n",
    "    # plt.ylabel(\"samples\")\n",
    "    # plt.legend(loc='upper left')\n",
    "    # # plt.savefig(\"GAN_hist_real_gen.png\", facecolor=\"white\")\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.ylim(0, 120)\n",
    "    # plt.hist(real_list, range=(0, 40), bins = 30, alpha = 0.5, label=\"real images\")\n",
    "    # plt.hist(list1, range=(0, 40), bins = 30, alpha = 0.5, label=\"1e-6\")\n",
    "    # plt.hist(list2, range=(0, 40), bins = 30, alpha = 0.5, label=\"1e-7\")\n",
    "    # plt.xlabel(\"estimated resistance\")\n",
    "    # plt.ylabel(\"samples\")\n",
    "    # plt.legend(loc='upper left')\n",
    "    # # plt.savefig(\"GAN_hist_real_gen+reg.png\", facecolor=\"white\")\n",
    "    # plt.show()\n",
    "\n",
    "# z = Variable(torch.randn(26 * 5, z_size, 1, 1)).to(device)\n",
    "\n",
    "# # Labels 0 ~ 9\n",
    "# labels = Variable(torch.LongTensor([i for _ in range(5) for i in range(class_num)])).to(device)\n",
    "\n",
    "# sample_images2 = generator2(z, labels)\n",
    "\n",
    "# sample_images2 = sample_images2.data.cpu()\n",
    "# grid = make_grid(sample_images2, nrow=26, normalize=True).permute(1,2,0).numpy()\n",
    "# fig, ax = plt.subplots(figsize=(26,26))\n",
    "# plt.imshow(grid)\n",
    "# plt.savefig('generator+reg_hinge25.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.xlim(0, 40)\n",
    "# plt.ylim(0, 3500)\n",
    "# plt.hist(all_real_list, range=(0, 40), bins = 40, alpha = 0.5, label=\"Real images\")\n",
    "# plt.hist(all_list1, range=(0, 40), bins = 40, alpha = 0.5, label=\"Reconstructed images\")\n",
    "# plt.tick_params(labelbottom=False, labelleft=False, labelright=False, labeltop=False)\n",
    "# plt.tick_params(bottom=False, left=False, right=False, top=False)\n",
    "# plt.xticks([k * 10 for k in range(5)])\n",
    "# plt.yticks([i * 500 for i in range(7)])\n",
    "# plt.grid()\n",
    "# plt.savefig(\"../GAN+classifier_c1e-6/hist_all.png\", facecolor=\"white\", bbox_inches='tight', pad_inches=0.1)\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../GAN+classifier_c1e-6/defensibility/{}.csv\".format(1), 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(np.array(all_list1))\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_images2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mセル18 を /workspace/src/attacked_examples.ipynb\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/attacked_examples.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m grid \u001b[39m=\u001b[39m make_grid(sample_images2, nrow\u001b[39m=\u001b[39m\u001b[39m26\u001b[39m, normalize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/attacked_examples.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(figsize\u001b[39m=\u001b[39m(\u001b[39m26\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6d795f646f636b6572227d/workspace/src/attacked_examples.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(grid)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_images2' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "grid = make_grid(sample_images2, nrow=26, normalize=True).permute(1,2,0).numpy()\n",
    "fig, ax = plt.subplots(figsize=(26,10))\n",
    "plt.imshow(grid)\n",
    "# plt.savefig('generator+reg_hinge25.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 1, 64, 64])\n",
      "4993\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADrCAYAAABXYUzjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAADfklEQVR4nO3YsW1CUQxA0f+ijAB1/v6zwBDUyQ5OjyiCBLkSnFNaLlzdwmtmNgD+30d9AMC7EmCAiAADRAQYICLAABEBBoh83rN8OBxm3/cnnQLwms7n88/MHK/ndwV43/ftdDo97iqAN7DWutyae0EARAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQWTPz9+W1vrdtuzzvHICX9DUzx+vhXQEG4HG8IAAiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiv24jG3fvmQi6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# z = Variable(torch.randn(5000, z_size, 1, 1)).to(device)\n",
    "\n",
    "# # Labels 0 ~ 9\n",
    "# labels = Variable(torch.LongTensor(np.zeros(5000) + 3)).to(device)\n",
    "\n",
    "# sample_images3 = generator2(z, labels)\n",
    "\n",
    "# sample_images3 = sample_images3.data.cpu()\n",
    "# print(sample_images3.shape)\n",
    "# for k in [\"strong\", \"soso\", \"weak\"]:\n",
    "#     for c in [chr(i) for i in range(65, 65+26)]:\n",
    "#             os.makedirs(\"gen_result/\" + k + \"/\" + c, exist_ok=True)\n",
    "# count = 0\n",
    "# for index, (image, label) in enumerate(zip(sample_images3, labels)):\n",
    "    \n",
    "#     image, label = image.unsqueeze(0).to(device), label.unsqueeze(0).type(torch.LongTensor).to(device)\n",
    "\n",
    "#     # データをモデルに順伝播させます\n",
    "#     output = model(image)\n",
    "#     init_pred = output.max(1, keepdim=True)[1] # 最大の確率のインデックスを取得します。\n",
    "\n",
    "#     # 最初から予測が間違っている場合、攻撃する必要がないため次のイテレーションに進みます。\n",
    "    \n",
    "#     if init_pred.item() != label.item():\n",
    "#         false += 1\n",
    "#         continue\n",
    "#     count += 1\n",
    "#     if rmodel_list[label.item()](image).item() > 25:\n",
    "#         plt.xticks([], [])\n",
    "#         plt.yticks([], [])\n",
    "#         plt.imsave(\"gen_result/strong/\" + chr(init_pred.item() + 65) + \"/{}.png\".format(index), image.squeeze().detach().cpu().numpy(), cmap=\"gray\")\n",
    "#     elif rmodel_list[label.item()](image).item() > 20:\n",
    "#         plt.xticks([], [])\n",
    "#         plt.yticks([], [])\n",
    "#         plt.imsave(\"gen_result/soso/\" + chr(init_pred.item() + 65) + \"/{}.png\".format(index), image.squeeze().detach().cpu().numpy(), cmap=\"gray\")\n",
    "#     elif rmodel_list[label.item()](image).item() <= 20:\n",
    "#         plt.xticks([], [])\n",
    "#         plt.yticks([], [])\n",
    "#         plt.imsave(\"gen_result/weak/\" + chr(init_pred.item() + 65) + \"/{}.png\".format(index), image.squeeze().detach().cpu().numpy(), cmap=\"gray\")\n",
    "#     reg_list1.append(rmodel_list[label.item()](image).item())\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = make_grid(sample_images, nrow=26, normalize=True).permute(1,2,0).numpy()\n",
    "# fig, ax = plt.subplots(figsize=(22,3))\n",
    "# plt.imshow(grid)\n",
    "# plt.savefig('generator+reg_hinge20.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
